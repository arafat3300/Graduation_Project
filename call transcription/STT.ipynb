{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_1DqaDtfbtS",
        "outputId": "8d47ee3b-c814-4417-b70a-5cd8baf5a3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/95.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q fastapi uvicorn nest-asyncio pyngrok aiohttp pydub torch numpy transformers faster-whisper openai\n",
        "!apt-get -y install ffmpeg\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-multipart\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyiuK-pUftC0",
        "outputId": "0822b911-3310-46e3-cb36-f782321f25ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ 1. Setup Auth and Imports ============\n",
        "from pyngrok import conf\n",
        "conf.get_default().auth_token = \"2wUYAZ80lQ4oSi5LXy6I3d59qMi_2aEJdorZD6BKAyyHLS3fk\"\n",
        "\n",
        "import logging, re, requests\n",
        "from pathlib import Path\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import numpy as np\n",
        "import torch\n",
        "from fastapi import FastAPI, HTTPException, File, UploadFile, Form, Request\n",
        "from pydub import AudioSegment\n",
        "from faster_whisper import WhisperModel\n",
        "from openai import OpenAI\n",
        "import uvicorn\n",
        "\n",
        "# ============ 2. Logging Setup ============\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ============ 3. Init FastAPI and Models ============\n",
        "client = OpenAI(api_key=\"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\")\n",
        "app = FastAPI()\n",
        "VOICE_DIR = Path(\"voice_registry\")\n",
        "VOICE_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = WhisperModel(\"large-v3\", device=device, compute_type=\"float32\")\n",
        "\n",
        "EGYPTIAN_PROMPT = (\n",
        "    \"This is Egyptian Arabic speech. Use dialect words like 'aywa', 'la2', 'mesh', \"\n",
        "    \"'enta fein', '3ayez', 'hat', 'mashy'. Transcribe naturally as Egyptians speak.\"\n",
        ")\n",
        "\n",
        "# ============ 4. Helper Functions ============\n",
        "def extract_features(path, target=480_000):\n",
        "    audio = AudioSegment.from_file(path).set_channels(1).set_frame_rate(16000)\n",
        "    samples = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
        "    samples /= np.max(np.abs(samples))\n",
        "    return np.pad(samples, (0, max(0, target - len(samples))))[:target]\n",
        "\n",
        "def save_profile(emp_id, wav):\n",
        "    np.save(VOICE_DIR / f\"{emp_id}.npy\", extract_features(wav))\n",
        "\n",
        "def load_profile(emp_id):\n",
        "    p = VOICE_DIR / f\"{emp_id}.npy\"\n",
        "    return np.load(p) if p.exists() else None\n",
        "\n",
        "def gpt_rephrase_full(text):\n",
        "    system_prompt = (\n",
        "        \"You are a professional real estate sales assistant.\\n\"\n",
        "        \"Rephrase each line individually to be more polite, fluent, and domain-specific for real estate sales.\\n\"\n",
        "        \"Keep 'Client:' and 'Broker:' labels exactly as they are.\\n\"\n",
        "        \"Rewrite in clear business English.\\n\"\n",
        "    )\n",
        "    try:\n",
        "        logger.info(\"🧠 Sending to GPT (first 200 chars): %s\", text[:200])\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": text}\n",
        "            ],\n",
        "            temperature=0.4,\n",
        "            max_tokens=2000\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        logger.error(\"❌ GPT Error: %s\", str(e))\n",
        "        return f\"(GPT Error): {str(e)}\"\n",
        "\n",
        "# ============ 5. POST Endpoint ============\n",
        "@app.post(\"/transcribe_file\")\n",
        "async def transcribe_file(\n",
        "    request: Request,\n",
        "    file: UploadFile = File(...),\n",
        "    emp_id: str = Form(\"Broker\")\n",
        "):\n",
        "    logger.info(\"🔄 Request headers: %s\", request.headers)\n",
        "    logger.info(\"📁 Received file: %s\", file.filename)\n",
        "    logger.info(\"👤 emp_id: %s\", emp_id)\n",
        "\n",
        "    temp_path = \"temp.wav\"\n",
        "    with open(temp_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "\n",
        "    save_profile(emp_id, temp_path)\n",
        "    profile = load_profile(emp_id)\n",
        "    if profile is None:\n",
        "        raise HTTPException(status_code=404, detail=\"No voice profile saved.\")\n",
        "\n",
        "    params = dict(\n",
        "        beam_size=2, vad_filter=True, vad_parameters=dict(min_silence_duration_ms=500),\n",
        "        temperature=0.0, condition_on_previous_text=False, compression_ratio_threshold=2.4,\n",
        "        language=\"ar\", best_of=1, word_timestamps=False,\n",
        "        initial_prompt=EGYPTIAN_PROMPT\n",
        "    )\n",
        "\n",
        "    segments, info = model.transcribe(temp_path, **params)\n",
        "    feats = extract_features(temp_path)\n",
        "    seg_list, out = list(segments), []\n",
        "\n",
        "    for i, seg in enumerate(seg_list):\n",
        "        is_emp = False\n",
        "        if i % 2 == 0:\n",
        "            start, end = int(seg.start * 16000), int(seg.end * 16000)\n",
        "            chunk = feats[start:end]\n",
        "            chunk = np.pad(chunk, (0, len(profile) - len(chunk)))[:len(profile)]\n",
        "            sim = np.dot(chunk, profile) / (np.linalg.norm(chunk) * np.linalg.norm(profile) + 1e-6)\n",
        "            is_emp = sim > 0.5\n",
        "        out.append({\n",
        "            \"start\": seg.start, \"end\": seg.end,\n",
        "            \"speaker\": emp_id if is_emp else \"Client\",\n",
        "            \"text\": seg.text\n",
        "        })\n",
        "\n",
        "    full_text = \"\\n\".join(f\"[{s['start']:.2f}-{s['end']:.2f}] {s['speaker']}: {s['text']}\" for s in out)\n",
        "\n",
        "    # Translate\n",
        "    tseg, _ = model.transcribe(\n",
        "        temp_path, task=\"translate\", language=\"ar\",\n",
        "        beam_size=2, temperature=0.0, best_of=1,\n",
        "        vad_filter=True, initial_prompt=EGYPTIAN_PROMPT\n",
        "    )\n",
        "    translated = \"\\n\".join(\n",
        "        f\"[{t.start:.2f}-{t.end:.2f}] {out[i]['speaker'] if i < len(out) else 'Unknown'}: {t.text}\"\n",
        "        for i, t in enumerate(tseg)\n",
        "    )\n",
        "\n",
        "    cleaned_translation = re.sub(r\"\\[\\d+\\.\\d+-\\d+\\.\\d+\\]\\s+\", \"\", translated)\n",
        "    logger.info(\"🔤 Translated text: %s\", cleaned_translation[:300])\n",
        "\n",
        "    gpt_text = gpt_rephrase_full(cleaned_translation)\n",
        "    logger.info(\"📝 GPT output: %s\", gpt_text[:300])\n",
        "\n",
        "    llama_url = \"https://121a-156-213-55-10.ngrok-free.app/qa\"\n",
        "    try:\n",
        "        logger.info(\"📤 Sending to LLaMA: %s\", gpt_text[:300])\n",
        "        response = requests.post(llama_url, json={\"transcript\": gpt_text})\n",
        "        response.raise_for_status()\n",
        "        qa_answers = response.json().get(\"answers\", {})\n",
        "        logger.info(\"✅ LLaMA answers: %s\", qa_answers)\n",
        "    except Exception as e:\n",
        "        logger.error(\"❌ Error contacting LLaMA: %s\", str(e))\n",
        "        qa_answers = {\"error\": str(e)}\n",
        "\n",
        "    return {\n",
        "        \"status\": \"success\",\n",
        "        \"language\": info.language,\n",
        "        \"transcription\": full_text,\n",
        "        \"translation\": translated,\n",
        "        \"final_rephrased_text\": gpt_text,\n",
        "        \"qa_answers\": qa_answers\n",
        "    }\n",
        "\n",
        "# ============ 6. Start Public API ============\n",
        "nest_asyncio.apply()\n",
        "public_url = ngrok.connect(7860)\n",
        "print(\"🚀 Public API available at:\", public_url)\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=7860)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-j0XMHRfu7G",
        "outputId": "2b5994ec-982e-47cf-c633-cc83e555c1ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Public API available at: NgrokTunnel: \"https://ffd3-34-169-201-2.ngrok-free.app\" -> \"http://localhost:7860\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [310]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:7860 (Press CTRL+C to quit)\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-18' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     156.213.235.106:0 - \"POST /transcribe_file HTTP/1.1\" 200 OK\n",
            "INFO:     156.213.235.106:0 - \"POST /transcribe_file HTTP/1.1\" 200 OK\n",
            "INFO:     156.213.235.106:0 - \"POST /transcribe_file HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    }
  ]
}